{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1550f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, mean_squared_error\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA, SparsePCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39ff8a",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a00bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Data = pd.read_csv('ED-sepsis.csv')\n",
    "death = S_Data.death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to training and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(S_Data, death, stratify=death, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babc721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create second dataset for 28-days mortality\n",
    "X_train_28 = X_train.copy()\n",
    "X_train_28.loc[X_train_28.HospTime > 28, 'death'] = 0\n",
    "\n",
    "X_val_28 = X_val.copy()\n",
    "X_val_28.loc[X_val_28.HospTime > 28, 'death'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = X_train[['EDTime', 'EDTime_grp','HospTime', 'death']].copy()\n",
    "\n",
    "X_train.drop(['EDTime'], axis=1, inplace= True)\n",
    "X_train.drop(['EDTime_grp'], axis=1, inplace= True)\n",
    "X_train.drop(['No'], axis=1, inplace= True)\n",
    "X_train.drop(['HospTime'], axis=1, inplace= True)\n",
    "X_train.drop(['death'], axis=1, inplace= True)\n",
    "X_train.drop(['WHITE'], axis=1, inplace= True)\n",
    "X_train.drop(['ASIAN'], axis=1, inplace= True)\n",
    "X_train.drop(['HISPANIC OR LATINO'], axis=1, inplace= True)\n",
    "X_train.drop(['BLACK'], axis=1, inplace= True)\n",
    "X_train.drop(['OTHER'], axis=1, inplace= True)\n",
    "\n",
    "Y_val = X_val[['EDTime', 'EDTime_grp','HospTime', 'death']].copy()\n",
    "\n",
    "X_val.drop(['EDTime'], axis=1, inplace= True)\n",
    "X_val.drop(['EDTime_grp'], axis=1, inplace= True)\n",
    "X_val.drop(['No'], axis=1, inplace= True)\n",
    "X_val.drop(['HospTime'], axis=1, inplace= True)\n",
    "X_val.drop(['death'], axis=1, inplace= True)\n",
    "X_val.drop(['WHITE'], axis=1, inplace= True)\n",
    "X_val.drop(['ASIAN'], axis=1, inplace= True)\n",
    "X_val.drop(['HISPANIC OR LATINO'], axis=1, inplace= True)\n",
    "X_val.drop(['BLACK'], axis=1, inplace= True)\n",
    "X_val.drop(['OTHER'], axis=1, inplace= True)\n",
    "\n",
    "Y_train_28 = X_train_28[['EDTime', 'EDTime_grp','HospTime', 'death']].copy()\n",
    "Y_val_28 = X_val_28[['EDTime', 'EDTime_grp','HospTime', 'death']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f87d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA will reset the index. So, reset the index as well so we can match them afterwards.\n",
    "X_train.reset_index(drop = True, inplace = True)\n",
    "X_val.reset_index(drop = True, inplace = True)\n",
    "\n",
    "Y_train.reset_index(drop = True, inplace = True)\n",
    "Y_val.reset_index(drop = True, inplace = True)\n",
    "Y_train_28.reset_index(drop = True, inplace = True)\n",
    "Y_val_28.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3028c",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93f920",
   "metadata": {},
   "source": [
    "### No Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFS_X_train = X_train.copy()\n",
    "NFS_X_val = X_val.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb8cd0",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "PCA_X_train = pca.fit_transform(X_train)\n",
    "PCA_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(NFS_X_train.shape[1]), PCA_variance, alpha=0.5, label='feature variance', color = 'red')\n",
    "plt.legend()\n",
    "plt.ylabel('Variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the number of variables so that the explained variance is at least 95%.\n",
    "var_exp = 0\n",
    "for i in range(NFS_X_train.shape[1]):\n",
    "    var_exp = var_exp + PCA_variance[i]\n",
    "    if var_exp >= 0.90:\n",
    "        num_var = i + 1\n",
    "        break\n",
    "print('To explain 90% of the data, we need ' + str(num_var) + ' variables from PCA.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9977c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca90 = PCA(n_components = num_var)\n",
    "pca90 = pca90.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a857d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_X_train = pd.DataFrame(pca90.transform(X_train))\n",
    "PCA_X_val = pd.DataFrame(pca90.transform(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97063a",
   "metadata": {},
   "source": [
    "### SPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = SparsePCA()\n",
    "SPA_X_train = spa.fit_transform(X_train)\n",
    "SPA_variance = np.var(SPA_X_train, axis=0)\n",
    "SPA_variance = SPA_variance / np.sum(SPA_variance)\n",
    "SPA_variance = -np.sort(-SPA_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52292233",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(NFS_X_train.shape[1]), SPA_variance, alpha=0.5, label='feature variance', color = 'red')\n",
    "plt.legend()\n",
    "plt.ylabel('Variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the number of variables so that the explained variance is at least 95%.\n",
    "var_exp = 0\n",
    "for i in range(NFS_X_train.shape[1]):\n",
    "    var_exp = var_exp + SPA_variance[i]\n",
    "    if var_exp >= 0.90:\n",
    "        num_var = i + 1\n",
    "        break\n",
    "print('To explain 90% of the data, we need ' + str(num_var) + ' variables from PCA.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa90 = SparsePCA(n_components=num_var)\n",
    "spa90 = spa90.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPA_X_train = pd.DataFrame(spa90.transform(X_train))\n",
    "SPA_X_val = pd.DataFrame(spa90.transform(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d642f",
   "metadata": {},
   "source": [
    "### Simple Logistic Regression\n",
    "We will perform a simple logistic regression using all the columns provided.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogR(X_train, Y_train, X_val, Y_val):\n",
    "    #Train the logistic regression model\n",
    "    LogR = LogisticRegression()\n",
    "    LogR.fit(X_train, Y_train)\n",
    "    Y_hat_train = LogR.predict(X_train)\n",
    "    Y_hat_val = LogR.predict(X_val)\n",
    "    \n",
    "    #classificiation report on the training and the validation set\n",
    "    Cl_Rep_tr = classification_report(Y_train, Y_hat_train)\n",
    "    print(\"Classification Report on the Training Set\")\n",
    "    print(Cl_Rep_tr)\n",
    "    Cl_Rep_val = classification_report(Y_val, Y_hat_val)\n",
    "    print(\"\\nClassification Report on the Validation Set\")\n",
    "    print(Cl_Rep_val)\n",
    "    \n",
    "    #Below is the coefficient of the trained logistic regression model\n",
    "    LogR_coef = pd.DataFrame(zip(X_train.columns, np.transpose(LogR.coef_)), columns=['features', 'coef'])\n",
    "    print(LogR_coef.loc[LogR_coef['features'] == 'EDTime'])\n",
    "    \n",
    "    return LogR_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12fde9a",
   "metadata": {},
   "source": [
    "### Logistic Regression: No Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ff25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFS_X_train.insert(0, 'EDTime', Y_train['EDTime'])\n",
    "NFS_X_val.insert(0, 'EDTime', Y_val['EDTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR_NFS_coef = LogR(NFS_X_train, Y_train.death, NFS_X_val, Y_val.death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR_NFS_coef_28 = LogR(NFS_X_train, Y_train_28.death, NFS_X_val, Y_val_28.death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5939b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFS_X = NFS_X_train.append(NFS_X_val, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b812e",
   "metadata": {},
   "source": [
    "### Logistic Regression: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_X_train.insert(0, 'EDTime', Y_train['EDTime'])\n",
    "PCA_X_val.insert(0, 'EDTime', Y_val['EDTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR_PCA_coef = LogR(PCA_X_train, Y_train.death, PCA_X_val, Y_val.death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR_PCA_coef_28 = LogR(PCA_X_train, Y_train_28.death, PCA_X_val, Y_val_28.death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_X = PCA_X_train.append(PCA_X_val, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b583b65",
   "metadata": {},
   "source": [
    "### Logistic Regression: SPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e7552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPA_X_train.insert(0, 'EDTime', Y_train['EDTime'])\n",
    "SPA_X_val.insert(0, 'EDTime', Y_val['EDTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR_SPA_coef = LogR(SPA_X_train, Y_train.death, SPA_X_val, Y_val.death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b798ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR_SPA_coef_28 = LogR(SPA_X_train, Y_train_28.death, SPA_X_val, Y_val_28.death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0040b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPA_X = SPA_X_train.append(SPA_X_val, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c6829",
   "metadata": {},
   "source": [
    "### Case Matching Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a39f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y_train.append(Y_val, ignore_index = True)\n",
    "Y.reset_index(drop = True, inplace = True)\n",
    "Y.insert(2, 'No', Y.index)\n",
    "\n",
    "Y_28 = Y_train_28.append(Y_val_28, ignore_index = True)\n",
    "Y_28.reset_index(drop = True, inplace = True)\n",
    "Y_28.insert(2, 'No', Y_28.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dict_death = {}\n",
    "for i in range(Y.shape[0]):\n",
    "    Y_dict_death[Y.iloc[i]['No']] = Y.iloc[i]['death']\n",
    "    \n",
    "Y_dict_death_28 = {}\n",
    "for i in range(Y_28.shape[0]):\n",
    "    Y_dict_death_28[Y_28.iloc[i]['No']] = Y_28.iloc[i]['death']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf998f",
   "metadata": {},
   "source": [
    "### Case Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a751509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psmpy import PsmPy\n",
    "from psmpy.functions import cohenD\n",
    "from psmpy.plotting import *\n",
    "from itertools import combinations\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFS_X.insert(0, 'EDTime_grp', Y['EDTime_grp'])\n",
    "NFS_X.insert(0, 'No', NFS_X.index)\n",
    "NFS_X.drop(['EDTime'], axis=1, inplace= True)\n",
    "\n",
    "PCA_X.insert(0, 'EDTime_grp', Y['EDTime_grp'])\n",
    "PCA_X.insert(0, 'No', PCA_X.index)\n",
    "PCA_X.drop(['EDTime'], axis=1, inplace= True)\n",
    "\n",
    "SPA_X.insert(0, 'EDTime_grp', Y['EDTime_grp'])\n",
    "SPA_X.insert(0, 'No', SPA_X.index)\n",
    "SPA_X.drop(['EDTime'], axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CaseMatch(X, Grp1, Grp2, Grp_Remove_1, Grp_Remove_2, Grp_Remove_3, Weight, bal, caliper):\n",
    "    #Remove all treatment except 2 groups\n",
    "    X = X.drop(X[X.EDTime_grp == Grp_Remove_1].index)\n",
    "    X = X.drop(X[X.EDTime_grp == Grp_Remove_2].index)\n",
    "    X = X.drop(X[X.EDTime_grp == Grp_Remove_3].index)\n",
    "\n",
    "    X = X.replace({'EDTime_grp':{Grp1:0, Grp2:1}})\n",
    "    \n",
    "    for i in range(1, Weight.shape[0]):\n",
    "        X.iloc[:, i + 1] = X.iloc[:, i + 1] * Weight.iloc[i,1][0]\n",
    "    \n",
    "    psm = PsmPy(X, treatment='EDTime_grp', indx='No', exclude = [])\n",
    "    psm.logistic_ps(balance = bal)\n",
    "    psm.predicted_data;\n",
    "    \n",
    "    psm.knn_matched(matcher = 'propensity_logit', replacement=False, caliper=caliper)\n",
    "    \n",
    "    X_M = psm.df_matched[['No', 'matched_ID']].copy()\n",
    "    \n",
    "    return X_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_NFS = LogR_NFS_coef.copy()\n",
    "for i in range(ones_NFS.shape[0]):\n",
    "    ones_NFS.iloc[i,1]=[1]\n",
    "\n",
    "ones_PCA = LogR_PCA_coef.copy()\n",
    "for i in range(ones_PCA.shape[0]):\n",
    "    ones_PCA.iloc[i,1]=[1]\n",
    "\n",
    "ones_SPA = LogR_SPA_coef.copy()\n",
    "for i in range(ones_SPA.shape[0]):\n",
    "    ones_SPA.iloc[i,1]=[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pval(a, b):\n",
    "    a = a.dropna()\n",
    "    b = b.dropna()\n",
    "    t, p = ttest_ind(a, b, alternative = 'less')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab773a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Match for each pair of group\n",
    "def GroupMatch(X, Weight, caliper, Y_dict):\n",
    "    try:\n",
    "        X_12 = CaseMatch(X, 1, 2, 3, 4, 5, Weight, True, caliper)\n",
    "    except:\n",
    "        X_12 = CaseMatch(X, 1, 2, 3, 4, 5, Weight, False, caliper)\n",
    "    \n",
    "    try:\n",
    "        X_13 = CaseMatch(X, 1, 3, 2, 4, 5, Weight, True, caliper)\n",
    "    except:\n",
    "        X_13 = CaseMatch(X, 1, 3, 2, 4, 5, Weight, False, caliper)\n",
    "        \n",
    "    try:\n",
    "        X_14 = CaseMatch(X, 1, 4, 2, 3, 5, Weight, True, caliper)\n",
    "    except:\n",
    "        X_14 = CaseMatch(X, 1, 4, 2, 3, 5, Weight, False, caliper)\n",
    "        \n",
    "    try:\n",
    "        X_15 = CaseMatch(X, 1, 5, 2, 3, 4, Weight, True, caliper)\n",
    "    except:\n",
    "        X_15 = CaseMatch(X, 1, 5, 2, 3, 4, Weight, False, caliper)\n",
    "        \n",
    "    try:\n",
    "        X_23 = CaseMatch(X, 2, 3, 1, 4, 5, Weight, True, caliper)\n",
    "    except:\n",
    "        X_23 = CaseMatch(X, 2, 3, 1, 4, 5, Weight, False, caliper)\n",
    "        \n",
    "    try:\n",
    "        X_24 = CaseMatch(X, 2, 4, 1, 3, 5, Weight, True, caliper)\n",
    "    except:\n",
    "        X_24 = CaseMatch(X, 2, 4, 1, 3, 5, Weight, False, caliper)\n",
    "        \n",
    "    try:\n",
    "        X_25 = CaseMatch(X, 2, 5, 1, 3, 4, Weight, True, caliper)\n",
    "    except:\n",
    "        X_25 = CaseMatch(X, 2, 5, 1, 3, 4, Weight, False, caliper)\n",
    "        \n",
    "    try:\n",
    "        X_34 = CaseMatch(X, 3, 4, 1, 2, 5, Weight, True, caliper)\n",
    "    except:\n",
    "        X_34 = CaseMatch(X, 3, 4, 1, 2, 5, Weight, False, caliper)\n",
    "        \n",
    "    try:\n",
    "        X_35 = CaseMatch(X, 3, 5, 1, 2, 4, Weight, True, caliper)\n",
    "    except:\n",
    "        X_35 = CaseMatch(X, 3, 5, 1, 2, 4, Weight, False, caliper)\n",
    "        \n",
    "    try:\n",
    "        X_45 = CaseMatch(X, 4, 5, 1, 2, 3, Weight, True, caliper)\n",
    "    except:\n",
    "        X_45 = CaseMatch(X, 4, 5, 1, 2, 3, Weight, False, caliper)\n",
    "    \n",
    "    X = []\n",
    "    \n",
    "    X_12 = X_12.dropna()\n",
    "    X_12['No_death'] = X_12['No'].apply(lambda x: Y_dict[x])\n",
    "    X_12['matched_ID_death'] = X_12['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_13 = X_13.dropna()\n",
    "    X_13['No_death'] = X_13['No'].apply(lambda x: Y_dict[x])\n",
    "    X_13['matched_ID_death'] = X_13['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_14 = X_14.dropna()\n",
    "    X_14['No_death'] = X_14['No'].apply(lambda x: Y_dict[x])\n",
    "    X_14['matched_ID_death'] = X_14['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_15 = X_15.dropna()\n",
    "    X_15['No_death'] = X_15['No'].apply(lambda x: Y_dict[x])\n",
    "    X_15['matched_ID_death'] = X_15['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_23 = X_23.dropna()\n",
    "    X_23['No_death'] = X_23['No'].apply(lambda x: Y_dict[x])\n",
    "    X_23['matched_ID_death'] = X_23['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_24 = X_24.dropna()\n",
    "    X_24['No_death'] = X_24['No'].apply(lambda x: Y_dict[x])\n",
    "    X_24['matched_ID_death'] = X_24['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_25 = X_25.dropna()\n",
    "    X_25['No_death'] = X_25['No'].apply(lambda x: Y_dict[x])\n",
    "    X_25['matched_ID_death'] = X_25['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_34 = X_34.dropna()\n",
    "    X_34['No_death'] = X_34['No'].apply(lambda x: Y_dict[x])\n",
    "    X_34['matched_ID_death'] = X_34['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_35 = X_35.dropna()\n",
    "    X_35['No_death'] = X_35['No'].apply(lambda x: Y_dict[x])\n",
    "    X_35['matched_ID_death'] = X_35['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    X_45 = X_45.dropna()\n",
    "    X_45['No_death'] = X_45['No'].apply(lambda x: Y_dict[x])\n",
    "    X_45['matched_ID_death'] = X_45['matched_ID'].apply(lambda x: Y_dict[x])\n",
    "    \n",
    "    X.append(X_12)\n",
    "    X.append(X_13)\n",
    "    X.append(X_14)\n",
    "    X.append(X_15)\n",
    "    X.append(X_23)\n",
    "    X.append(X_24)\n",
    "    X.append(X_25)\n",
    "    X.append(X_34)\n",
    "    X.append(X_35)\n",
    "    X.append(X_45)\n",
    "    \n",
    "    num = []\n",
    "    mor1 = []\n",
    "    mor2 = []\n",
    "    pval = []\n",
    "    for i in range(10):\n",
    "        num.append(X[i].shape[0])\n",
    "        mor1.append(sum(X[i]['No_death'])/num[i])\n",
    "        mor2.append(sum(X[i]['matched_ID_death'])/num[i])\n",
    "        pval.append(check_pval(X[i]['No_death'], X[i]['matched_ID_death']))\n",
    "        \n",
    "    X_summary = pd.DataFrame()\n",
    "    X_summary['Group 1'] = [1, 1, 1, 1, 2, 2, 2, 3, 3, 4]\n",
    "    X_summary['Group 2'] = [2, 3, 4, 5, 3, 4, 5, 4, 5, 5]\n",
    "    X_summary['Mortality Rate 1'] = mor1\n",
    "    X_summary['Mortality Rate 2'] = mor2\n",
    "    X_summary['Number of cases'] = num\n",
    "    X_summary['P-value'] = pval\n",
    "\n",
    "    \n",
    "    return X_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46750c0e",
   "metadata": {},
   "source": [
    "### No Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe3ff8",
   "metadata": {},
   "source": [
    "Without caliper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae65d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without Weight\n",
    "NFS_M_NW_nocal = GroupMatch(NFS_X, ones_NFS, None, Y_dict_death)\n",
    "NFS_M_NW_28_nocal = GroupMatch(NFS_X, ones_NFS, None, Y_dict_death_28)\n",
    "#With Weight\n",
    "NFS_M_GM_nocal = GroupMatch(NFS_X, LogR_NFS_coef, None, Y_dict_death)\n",
    "NFS_M_28_nocal = GroupMatch(NFS_X, LogR_NFS_coef_28, None, Y_dict_death_28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f0a3a",
   "metadata": {},
   "source": [
    "With caliper = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without Weight\n",
    "NFS_M_NW_cal02 = GroupMatch(NFS_X, ones_NFS, 0.2, Y_dict_death)\n",
    "NFS_M_NW_28_cal02 = GroupMatch(NFS_X, ones_NFS, 0.2, Y_dict_death_28)\n",
    "#With Weight\n",
    "NFS_M_GM_cal02 = GroupMatch(NFS_X, LogR_NFS_coef, 0.2, Y_dict_death)\n",
    "NFS_M_28_cal02 = GroupMatch(NFS_X, LogR_NFS_coef_28, 0.2, Y_dict_death_28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf8935",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbd544",
   "metadata": {},
   "source": [
    "Without caliper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100978a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without Weight\n",
    "PCA_M_NW_nocal = GroupMatch(PCA_X, ones_PCA, None, Y_dict_death)\n",
    "PCA_M_NW_28_nocal = GroupMatch(PCA_X, ones_PCA, None, Y_dict_death_28)\n",
    "#With Weight\n",
    "PCA_M_GM_nocal = GroupMatch(PCA_X, LogR_PCA_coef, None, Y_dict_death)\n",
    "PCA_M_28_nocal = GroupMatch(PCA_X, LogR_PCA_coef_28, None, Y_dict_death_28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae5b3e",
   "metadata": {},
   "source": [
    "With caliper = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without Weight\n",
    "PCA_M_NW_cal02 = GroupMatch(PCA_X, ones_PCA, 0.2, Y_dict_death)\n",
    "PCA_M_NW_28_cal02 = GroupMatch(PCA_X, ones_PCA, 0.2, Y_dict_death_28)\n",
    "#With Weight\n",
    "PCA_M_GM_cal02 = GroupMatch(PCA_X, LogR_PCA_coef, 0.2, Y_dict_death)\n",
    "PCA_M_28_cal02 = GroupMatch(PCA_X, LogR_PCA_coef_28, 0.2, Y_dict_death_28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e48130",
   "metadata": {},
   "source": [
    "### SPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726283d9",
   "metadata": {},
   "source": [
    "Without caliper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without Weight\n",
    "SPA_M_NW_nocal = GroupMatch(SPA_X, ones_SPA, None, Y_dict_death)\n",
    "SPA_M_NW_28_nocal = GroupMatch(SPA_X, ones_SPA, None, Y_dict_death_28)\n",
    "#With Weight\n",
    "SPA_M_GM_nocal = GroupMatch(SPA_X, LogR_SPA_coef, None, Y_dict_death)\n",
    "SPA_M_28_nocal = GroupMatch(SPA_X, LogR_SPA_coef_28, None, Y_dict_death_28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893dc24",
   "metadata": {},
   "source": [
    "With caliper = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ea4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without Weight\n",
    "SPA_M_NW_cal02 = GroupMatch(SPA_X, ones_SPA, 0.2, Y_dict_death)\n",
    "SPA_M_NW_28_cal02 = GroupMatch(SPA_X, ones_SPA, 0.2, Y_dict_death_28)\n",
    "#With Weight\n",
    "SPA_M_GM_cal02 = GroupMatch(SPA_X, LogR_SPA_coef, 0.2, Y_dict_death)\n",
    "SPA_M_28_cal02 = GroupMatch(SPA_X, LogR_SPA_coef_28, 0.2, Y_dict_death_28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d920f6",
   "metadata": {},
   "source": [
    "## Export results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d79fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Summary.xlsx') as writer:\n",
    "    NFS_M_NW_nocal.to_excel(writer, sheet_name='NFS_M_NW_nocal')\n",
    "    NFS_M_NW_28_nocal.to_excel(writer, sheet_name='NFS_M_NW_28_nocal')\n",
    "    NFS_M_GM_nocal.to_excel(writer, sheet_name='NFS_M_GM_nocal')\n",
    "    NFS_M_28_nocal.to_excel(writer, sheet_name='NFS_M_28_nocal')\n",
    "    NFS_M_NW_cal02.to_excel(writer, sheet_name='NFS_M_NW_cal02')\n",
    "    NFS_M_NW_28_cal02.to_excel(writer, sheet_name='NFS_M_NW_28_cal02')\n",
    "    NFS_M_GM_cal02.to_excel(writer, sheet_name='NFS_M_GM_cal02')\n",
    "    NFS_M_28_cal02.to_excel(writer, sheet_name='NFS_M_28_cal02')\n",
    "    PCA_M_NW_nocal.to_excel(writer, sheet_name='PCA_M_NW_nocal')\n",
    "    PCA_M_NW_28_nocal.to_excel(writer, sheet_name='PCA_M_NW_28_nocal')\n",
    "    PCA_M_GM_nocal.to_excel(writer, sheet_name='PCA_M_GM_nocal')\n",
    "    PCA_M_28_nocal.to_excel(writer, sheet_name='PCA_M_28_nocal')\n",
    "    PCA_M_NW_cal02.to_excel(writer, sheet_name='PCA_M_NW_cal02')\n",
    "    PCA_M_NW_28_cal02.to_excel(writer, sheet_name='PCA_M_NW_28_cal02')\n",
    "    PCA_M_GM_cal02.to_excel(writer, sheet_name='PCA_M_GM_cal02')\n",
    "    PCA_M_28_cal02.to_excel(writer, sheet_name='PCA_M_28_cal02')\n",
    "    SPA_M_NW_nocal.to_excel(writer, sheet_name='SPA_M_NW_nocal')\n",
    "    SPA_M_NW_28_nocal.to_excel(writer, sheet_name='SPA_M_NW_28_nocal')\n",
    "    SPA_M_GM_nocal.to_excel(writer, sheet_name='SPA_M_GM_nocal')\n",
    "    SPA_M_28_nocal.to_excel(writer, sheet_name='SPA_M_28_nocal')\n",
    "    SPA_M_NW_cal02.to_excel(writer, sheet_name='SPA_M_NW_cal02')\n",
    "    SPA_M_NW_28_cal02.to_excel(writer, sheet_name='SPA_M_NW_28_cal02')\n",
    "    SPA_M_GM_cal02.to_excel(writer, sheet_name='SPA_M_GM_cal02')\n",
    "    SPA_M_28_cal02.to_excel(writer, sheet_name='SPA_M_28_cal02')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
